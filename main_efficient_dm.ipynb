{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hZLFJuX3_vID"
   },
   "source": [
    "# Deep Reinforcement Learning — Doom Agent (SS2025)\n",
    "\n",
    "Welcome to the last assignment for the **Deep Reinforcement Learning** course (SS2025). In this notebook, you\"ll implement and train a reinforcement learning agent to play **Doom**.\n",
    "\n",
    "You will:\n",
    "- Set up a custom VizDoom environment with shaped rewards\n",
    "- Train an agent using an approach of your choice\n",
    "- Track reward components across episodes\n",
    "- Evaluate the best model\n",
    "- Visualize performance with replays and GIFs\n",
    "- Export the trained agent to ONNX to submit to the evaluation server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mi5FBK3oi1lI",
    "outputId": "6a38df99-16cd-4a65-8bb0-ead70efcbd71"
   },
   "outputs": [],
   "source": [
    "# Clone repo\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%cd /content/drive/MyDrive/Colab\\ Notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repo\n",
    "#!git clone https://github.com/albortino/doom-drl-challenge.git\n",
    "#%cd doom-drl-challenge\n",
    "\n",
    "# Install the dependencies\n",
    "#!pip install torch numpy matplotlib vizdoom portpicker gym onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import get_ipython\n",
    "get_ipython().run_line_magic('load_ext', 'autoreload')\n",
    "get_ipython().run_line_magic('autoreload', '2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8VR5QUw8ieGh"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from collections import deque\n",
    "from copy import deepcopy\n",
    "import random\n",
    "from torch import nn\n",
    "from doom_arena import VizdoomMPEnv\n",
    "import numpy as np\n",
    "import torch \n",
    "import os\n",
    "from datetime import datetime\n",
    "from doom_arena.reward import VizDoomReward\n",
    "from collections import defaultdict\n",
    "from functools import singledispatchmethod\n",
    "from tqdm.notebook import trange\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from typing import Dict, List\n",
    "import torch\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from doom_arena.render import render_episode\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "import pandas as pd\n",
    "import contextlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TqdmProgress:\n",
    "    \"\"\"A tqdm progress bar wrapper for training loops.\"\"\"\n",
    "\n",
    "    def __init__(self, total: int, desc: str = \"Training\", unit: str = \"episode\"):\n",
    "        \"\"\"\n",
    "        Initializes the progress bar.\n",
    "\n",
    "        Args:\n",
    "            total (int): The total number of iterations (e.g., episodes).\n",
    "            desc (str): A description for the progress bar.\n",
    "            unit (str): The unit for one iteration.\n",
    "        \"\"\"\n",
    "        self._iterable_pbar = trange(total, total=total, desc=desc, unit=f\" {unit}\")\n",
    "        self.pbar = self._iterable_pbar\n",
    "        self._total_steps = 0\n",
    "\n",
    "    def __enter__(self):\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        self.close()\n",
    "\n",
    "    def __iter__(self):\n",
    "        yield from self._iterable_pbar\n",
    "\n",
    "    def update_step_count(self, steps: int = 1):\n",
    "        \"\"\"\n",
    "        Increments the total step counter.\n",
    "\n",
    "        Args:\n",
    "            steps (int): The number of steps to add.\n",
    "        \"\"\"\n",
    "        self._total_steps += steps\n",
    "\n",
    "    def set_description(self, current_episode: int):\n",
    "        \"\"\"\n",
    "        Updates the description of the progress bar.\n",
    "\n",
    "        Args:\n",
    "            current_episode (int): The current episode number (0-indexed).\n",
    "        \"\"\"\n",
    "        desc = f\"Episode {current_episode + 1}/{self.pbar.total} | Total Training Steps: {self._total_steps:,}\"\n",
    "        self.pbar.set_description(desc)\n",
    "\n",
    "    def set_postfix(self, stats: dict):\n",
    "        \"\"\"\n",
    "        Updates the postfix of the progress bar with training statistics.\n",
    "        \"\"\"\n",
    "        self.pbar.set_postfix(stats)\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"Closes the progress bar.\"\"\"\n",
    "        self.pbar.close()\n",
    "\n",
    "\n",
    "class YourReward(VizDoomReward):\n",
    "    def __init__(self, num_players: int):\n",
    "        super().__init__(num_players)\n",
    "        self.prev_ammo = {}\n",
    "        self.prev_health = {}\n",
    "        self.prev_position = {}\n",
    "        self.survival_bonus = 0\n",
    "        \n",
    "    def __call__(self, vizdoom_reward: float, game_var: dict[str, float], game_var_old: dict[str, float], player_id: int) -> tuple:\n",
    "        \"\"\"\n",
    "        Custom reward functions\n",
    "        * +100 for frags (kills)\n",
    "        * +10 for hits\n",
    "        * -2 for damage taken\n",
    "        * +3 for movement (exploration)\n",
    "        * +1 for ammo efficiency, +2 if ammo pickup\n",
    "        * +0.05 survival bonus per step, -20 if dead\n",
    "        * +2 if health pickup\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "            {'HEALTH': 100.0,  Can be higher then 100!\n",
    "            'AMMO3': 0.0,\n",
    "            'FRAGCOUNT': 0.0, # Number of players/bots killed, minus the number of committed suicides. Useful only in multiplayer mode\n",
    "            'ARMOR': 0.0,\n",
    "            'HITCOUNT': 0.0, # Counts number of hit monsters/players/bots during the current episode.\n",
    "            'HITS_TAKEN': 0.0, # Counts number of hits taken by the player during the current episode.\n",
    "            'DEAD': 0.0,\n",
    "            'DEATHCOUNT': 0.0, # Counts the number of players deaths during the current episode. Useful only in multiplayer mode.\n",
    "            'DAMAGECOUNT': 0.0, # Counts number of damage dealt to monsters/players/bots during the current episode. \n",
    "            'DAMAGE_TAKEN': 0.0,\n",
    "            'KILLCOUNT': 0.0,\n",
    "            'SELECTED_WEAPON': 2.0,\n",
    "            'SELECTED_WEAPON_AMMO': 94.0, # Ammo for selected weapon.\n",
    "            'POSITION_X': 389.96946716308594,\n",
    "            'POSITION_Y': 274.2670135498047}\n",
    "        \"\"\"\n",
    "        self._step += 1\n",
    "        _ = vizdoom_reward, player_id  # unused\n",
    "        \n",
    "        def calc_movement(pos_x, pos_y, pos_x_old, pos_y_old):\n",
    "            return np.sqrt((pos_x - pos_x_old)**2 + (pos_y - pos_y_old)**2).item() # Euclidean distance\n",
    "\n",
    "        # Combat reward from hits\n",
    "        rwd_frag = 100.0 * (game_var[\"FRAGCOUNT\"] - game_var_old[\"FRAGCOUNT\"])\n",
    "        rwd_hit = 5.0 * (game_var[\"DAMAGECOUNT\"] - game_var_old[\"DAMAGECOUNT\"])\n",
    "        rwd_hit_taken = -1 * (game_var[\"HITS_TAKEN\"] - game_var_old[\"HITS_TAKEN\"])\n",
    "        \n",
    "        # Movement reward\n",
    "        pos_x, pos_y = game_var.get(\"POSITION_X\", 0), game_var.get(\"POSITION_Y\", 0)\n",
    "        pos_x_old, pos_y_old = game_var_old.get(\"POSITION_X\", 0), game_var_old.get(\"POSITION_Y\", 0)\n",
    "                \n",
    "        movement_dist = calc_movement(pos_x, pos_y, pos_x_old, pos_y_old)\n",
    "        rwd_movement = 0 #0.05 * min(movement_dist / 100.0, 1.0) # Max movement factor is 1\n",
    "    \n",
    "        # Ammo efficiency\n",
    "        ammo_used = game_var_old.get(\"SELECTED_WEAPON_AMMO\", 0) - game_var.get(\"SELECTED_WEAPON_AMMO\", 0)\n",
    "        hits_made = game_var[\"HITCOUNT\"] - game_var_old[\"HITCOUNT\"]\n",
    "        \n",
    "        if ammo_used > 0: # Shots fired\n",
    "            accuracy = min(hits_made / ammo_used, 1.0)  # Cap at 100%\n",
    "            rwd_ammo_efficiency = 2.0 * accuracy\n",
    "    \n",
    "        elif ammo_used < 0: # Picked up ammunition\n",
    "            rwd_ammo_efficiency = 2.0\n",
    "            \n",
    "        else:\n",
    "            rwd_ammo_efficiency = 0.0\n",
    "            \n",
    "        # Survival bonus\n",
    "        health_ratio = game_var[\"HEALTH\"] / 100.0\n",
    "        is_alive = game_var[\"HEALTH\"] > 0\n",
    "        rwd_survival = (-0.01 + 0.005 * health_ratio) if is_alive else -20.0 # THe higher the health percentage the higher the reward, not linearly\n",
    "\n",
    "        # Bonus point if a reward is picked up\n",
    "        rwd_health_pickup = +5.0 if game_var[\"HEALTH\"] > game_var_old[\"HEALTH\"] else 0.0\n",
    "        \n",
    "        return rwd_frag, rwd_hit, rwd_hit_taken, rwd_movement, rwd_ammo_efficiency, rwd_survival, rwd_health_pickup\n",
    "\n",
    "class ExtraStates():\n",
    "    \"\"\" Class to store the state informations and calculations. \"\"\"\n",
    "    # States in correct order\n",
    "    EXTRA_STATE_INFOS = {\n",
    "        \"screen\": {\"dim\": 3, \"index\": 0},\n",
    "        \"labels\": {\"dim\": 1, \"index\": 3},\n",
    "        \"depth\": {\"dim\": 1, \"index\": 4},\n",
    "        \"automap\": {\"dim\": 3, \"index\": 5}\n",
    "        }\n",
    "    \n",
    "    def __init__(self, selected_states: list, num_frames: int = 1):\n",
    "        self.states = [key for key in self.EXTRA_STATE_INFOS.keys() if key in selected_states]\n",
    "        self.states_infos = {key: values for key, values in self.EXTRA_STATE_INFOS.items() if key in self.states}\n",
    "        self.num_states = len(self.states)\n",
    "        self.num_frames = num_frames\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.states)\n",
    "    \n",
    "    def get_state_info(self, info: str, return_dict: bool = True) -> dict|list:\n",
    "        if isinstance(info, str):\n",
    "            filtered_infos_dict = {key: infos.get(info, None) for key, infos in self.states_infos.items()}\n",
    "            \n",
    "            if return_dict:\n",
    "                return filtered_infos_dict\n",
    "        \n",
    "            else:\n",
    "                return list(filtered_infos_dict.values())\n",
    "    \n",
    "    def get_dims(self, return_dict: bool = True) -> dict|list:\n",
    "        return self.get_state_info(\"dim\", return_dict)\n",
    "    \n",
    "    def get_indices(self, return_dict: bool = True) -> dict|list:\n",
    "        return self.get_state_info(\"index\", return_dict)\n",
    "    \n",
    "    def get_channels(self, num_frames: int = None):\n",
    "        \"\"\" TODO: input_channels per state. Currently only for model initialization. \"\"\"\n",
    "        if num_frames is not None:\n",
    "            self.num_frames = num_frames\n",
    "            \n",
    "        return {k: v * self.num_frames for k, v in self.get_dims().items()}\n",
    "\n",
    "class EnvActions():\n",
    "    action_weights = {\n",
    "            'Noop': 0.05,\n",
    "            'Move Forward': 0.18,\n",
    "            'Attack': 0.18,\n",
    "            'Move Left': 0.10,\n",
    "            'Move Right': 0.10,\n",
    "            'Turn Left': 0.15,\n",
    "            'Turn Right': 0.15,\n",
    "            'Jump': 0.08}\n",
    "    \n",
    "    def __init__(self, env, seed: int = 149, rng = None) -> None:\n",
    "        self.set_actions_from_env(env)\n",
    "        self.action_space = len(self)\n",
    "        \n",
    "        if rng is not None:\n",
    "            self.rng = rng\n",
    "        elif seed is not None:\n",
    "            self.rng = np.random.default_rng(seed)\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.actions)\n",
    "    \n",
    "    def set_actions_from_env(self, env) -> None:\n",
    "        self.actions = {0: \"Noop\"}\n",
    "\n",
    "        for player_env in env.envs:\n",
    "            for idx, action in enumerate(player_env.game.get_available_buttons()):\n",
    "                action_name = str(action).split(\".\")[1].split(\":\")[0].replace(\"_\", \" \")\n",
    "                action_val = idx + 1\n",
    "                \n",
    "                self.actions[action_val] = action_name.title()\n",
    "    \n",
    "    def get_actions(self, return_vals_list: bool = False) -> dict|list:\n",
    "        if return_vals_list:\n",
    "            return list(self.actions.values())\n",
    "        \n",
    "        return self.actions\n",
    "    \n",
    "    def get_action_name(self, action_num: int) -> str:\n",
    "        return self.actions.get(action_num, \"Unknown Action\")\n",
    "    \n",
    "    @singledispatchmethod\n",
    "    def get_action_value(self, arg1, arg2):\n",
    "        \"\"\"Gets the action value(s) for the given index or list of indices.\"\"\"\n",
    "        raise NotImplementedError(f\"Cannot get action value for types {type(arg1)} and {type(arg2)}\")\n",
    "\n",
    "    @get_action_value.register\n",
    "    def _(self, index: int, num=1) -> int|list:    \n",
    "        all_buttons = list(self.actions.keys())\n",
    "    \n",
    "        if num == 1:\n",
    "            return all_buttons[index]\n",
    "        \n",
    "        return [all_buttons[index] for _ in range(num)]\n",
    "    \n",
    "    @get_action_value.register\n",
    "    def _(self, indices: list, num=1) -> list:\n",
    "        return [self.get_action_value(index) for index in indices]\n",
    "    \n",
    "    def get_action_proba(self) -> np.ndarray:\n",
    "        # Calculate the probability of each action\n",
    "        action_weight_vals = np.array(list(self.action_weights.values()))\n",
    "        action_proba = action_weight_vals / action_weight_vals.sum()\n",
    "        \n",
    "        return action_proba\n",
    "        \n",
    "    def get_random_action(self, n: int = 1, use_proba: bool = True) -> int | list:\n",
    "        if n <= 0:\n",
    "            return []\n",
    "\n",
    "        if use_proba:\n",
    "            action_proba = self.get_action_proba()\n",
    "            selection = self.rng.choice(list(self.actions.keys()), p=action_proba, size=n)\n",
    "        else:\n",
    "            selection = self.rng.choice(list(self.actions.keys()), size=n)\n",
    "\n",
    "        if n == 1:\n",
    "            return selection.item()\n",
    "        \n",
    "        return selection.tolist()\n",
    "    \n",
    "class ActionCounter:\n",
    "    \"\"\" Class to count the occurrences of each action. \"\"\"\n",
    "    def __init__(self):\n",
    "        self.counts = defaultdict(int)\n",
    "\n",
    "    def add(self, action: int):\n",
    "        \"\"\" Increments the count for a given action. \"\"\"\n",
    "        self.counts[action] += 1\n",
    "\n",
    "    def get_counts(self) -> dict[int, int]:\n",
    "        \"\"\" Returns the current action counts. \"\"\"\n",
    "        return dict(self.counts) # Return a regular dict\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\" Resets all action counts. \"\"\"\n",
    "        self.counts.clear()\n",
    "        \n",
    "    def get_name_counts(self, env_actions: EnvActions) -> dict[str, int]:\n",
    "        \"\"\" Returns the current action counts and names. \"\"\"\n",
    "        return {env_actions.get_action_name(key): self.counts.get(key, 0) for key in sorted(self.counts.keys())}\n",
    "        \n",
    "\n",
    "class LossLogger():\n",
    "    \"\"\" Class to log the losses during training or evaluation. \"\"\"\n",
    "    def __init__(self):\n",
    "        self.all_losses = list()\n",
    "        self.loss: float = 0.0\n",
    "        self.num_batches: int = 0\n",
    "        \n",
    "    def add(self, loss: torch.Tensor):\n",
    "        \"\"\" Adds the loss tensor. \"\"\"\n",
    "        self.loss += loss.detach().item()\n",
    "        self.num_batches += 1\n",
    "\n",
    "    def get_loss(self)-> float:\n",
    "        \"\"\" Returns the average loss as float. \"\"\"\n",
    "        if self.num_batches > 0:\n",
    "            return self.loss / self.num_batches\n",
    "        else:\n",
    "            return 0.0\n",
    "        \n",
    "    def reset_on_epoch(self):\n",
    "        \"\"\" Resets the losses after an epoch. \"\"\"\n",
    "        if self.num_batches > 0:\n",
    "            self.all_losses.append(self.get_loss())\n",
    "            self.loss = 0.0\n",
    "            self.num_batches = 0\n",
    "\n",
    "\n",
    "class Logger():\n",
    "    def __init__(self, path: str, filename: str = \"logs.txt\", also_print: bool = False):\n",
    "        \"\"\" Initialize a logger class that logs the messages to a file but is also capable of printing it.\n",
    "\n",
    "        Args:\n",
    "            path (str): Path to store the log file\n",
    "            also_print (bool, optional): Whether every log should also be printed (Tipp: self.log() allows for one-time printing, too!). Defaults to False.\n",
    "        \"\"\"\n",
    "       \n",
    "        self.path = path\n",
    "        self.filename = filename\n",
    "        self.also_print = also_print\n",
    "        self.file_path = os.path.join(self.path, self.filename)\n",
    "        \n",
    "        self.create_log_file()\n",
    "    \n",
    "    def log(self, msg: str, print_once: bool = False, improve_file_output: bool = False, end=\"\\n\"):\n",
    "        \n",
    "        if self.also_print or print_once:\n",
    "            print(msg)\n",
    "            \n",
    "        with open(self.file_path, \"a\") as f:\n",
    "            if improve_file_output:\n",
    "                msg = msg.replace(\"|\", \",\")\n",
    "                msg = msg.replace(\"\\t\", \"\")\n",
    "                msg = msg.replace(\"  \", \"\")\n",
    "                msg = msg.strip()\n",
    "            \n",
    "            f.write(msg + end)\n",
    "\n",
    "    def create_log_file(self):\n",
    "        # Ensure the directory exists\n",
    "        if not os.path.exists(self.path):\n",
    "            os.makedirs(self.path, exist_ok=True)\n",
    "            \n",
    "        with open(self.file_path, \"w\") as f:\n",
    "            f.write(F\"LOGGER INITIALIZED AT {datetime.now().strftime('%Y%m%d-%H%M%S')}\\n\")\n",
    "             \n",
    "            \n",
    "class ActivationLogger(Logger):\n",
    "    def __init__(self, path: str, filename: str = \"activations.txt\", also_print: bool = False):\n",
    "        super().__init__(path, filename, also_print)\n",
    "        \n",
    "    def analyze_activations(self, activations: torch.Tensor, episode: int = -1, title: str = \"\", print_once: bool = False) -> str:\n",
    "        return f\"Episode {episode} | {title:<15}| Shape: {list(activations.shape)},\\tMean: {activations.mean().item():.2f},\\tStd: {activations.std().item():.2f},\\tNorm: {torch.norm(activations).item():.2f}\"\n",
    "            \n",
    "    @torch.no_grad()\n",
    "    def log_model_activations(self, obs:tuple[torch.Tensor], model: torch.nn.Module, model_sequence: list = [None, 0, 1, 1], episode: int = -1, print_once: bool = False, return_activations_from_idx:  int = -1):       \n",
    "        \n",
    "        orig_device = next(model.parameters()).device\n",
    "        model.eval().cpu()\n",
    "        if isinstance(obs, tuple):\n",
    "            obs = tuple([o.cpu() for o in obs])\n",
    "        \n",
    "        elif isinstance(obs, list):\n",
    "            obs = [o.cpu() for o in obs]\n",
    "            \n",
    "        elif isinstance(obs, torch.Tensor):\n",
    "            obs = obs.cpu()\n",
    "            \n",
    "        # Get all modules ot the model except for activation functions        \n",
    "        all_modules = [name for name, _ in model.named_children() if name != \"phi\"]\n",
    "        \n",
    "        # Create a dictionary with relevant information that stores all data\n",
    "        module_info = {idx: {\"name\": module, \n",
    "                             \"sequence\": sequence, \n",
    "                             \"logits\": None} \n",
    "                       for idx, (module, sequence) in enumerate(zip(all_modules, model_sequence))}\n",
    "        \n",
    "        # Store all texts at one place\n",
    "        log_str = \"\"\n",
    "        \n",
    "        # Iterate over provided sequences and log \n",
    "        for module_idx, module_vals in module_info.items():\n",
    "            name = module_vals.get(\"name\", \"Unknown Module\") # Get the name\n",
    "            sequence = module_vals.get(\"sequence\") # Get the sequence, that is order where to retrieve from\n",
    "            module: torch.nn.Module = getattr(model, name) # Get the module as instance\n",
    "\n",
    "            if sequence is None:\n",
    "                module_input = obs\n",
    "            else:\n",
    "                module_input = module_info.get(sequence).get(\"logits\") # If module needs logits form previously get them\n",
    "            \n",
    "            module_logits = module.forward(module_input) # Make forward pass\n",
    "            module_info[module_idx][\"logits\"] = module_logits # Append forward pass to module info dictionary\n",
    "            \n",
    "            log_str += self.analyze_activations(module_logits, episode, name, print_once) + \"\\n\" # Analyze the logits\n",
    "            \n",
    "        # Log all values together\n",
    "        self.log(log_str, print_once, improve_file_output=True)\n",
    "\n",
    "        # Put model on original device\n",
    "        model.to(orig_device)\n",
    "        \n",
    "        # Return the last X activations if necessary\n",
    "        if return_activations_from_idx is not None:\n",
    "            return_idx = len(all_modules) + return_activations_from_idx if return_activations_from_idx < 0 else return_activations_from_idx\n",
    "            \n",
    "            all_indices = list(module_info.keys())\n",
    "            selected_indices = all_indices[return_idx:]\n",
    "            \n",
    "            return [value.get(\"logits\") for key, value in module_info.items() if key in selected_indices]\n",
    "        \n",
    "    def analyze_weights(self, weights: torch.Tensor, episode: int = -1, title: str = \"\", layer_type: str = \"unknown\", print_once: bool = False) -> str:\n",
    "        \"\"\"\n",
    "        Analyze weight statistics for a given layer\n",
    "        \n",
    "        Args:\n",
    "            weights: Weight tensor to analyze\n",
    "            episode: Episode number\n",
    "            title: Layer name/title\n",
    "            layer_type: Type of layer (conv, linear, etc.)\n",
    "            print_once: Whether to print once or always\n",
    "        \n",
    "        Returns:\n",
    "            Formatted string with weight statistics\n",
    "        \"\"\"\n",
    "        # Basic statistics\n",
    "        mean_val = weights.mean().item()\n",
    "        std_val = weights.std().item()\n",
    "        norm_val = torch.norm(weights).item()\n",
    "        \n",
    "        # Advanced statistics for training monitoring\n",
    "        min_val = weights.min().item()\n",
    "        max_val = weights.max().item()\n",
    "        abs_mean = weights.abs().mean().item()\n",
    "        \n",
    "        # Gradient flow indicators\n",
    "        near_zero_ratio = (weights.abs() < 1e-6).float().mean().item()\n",
    "        large_weight_ratio = (weights.abs() > 1.0).float().mean().item()\n",
    "        \n",
    "        # Weight distribution analysis\n",
    "        q25 = torch.quantile(weights.flatten(), 0.25).item()\n",
    "        q75 = torch.quantile(weights.flatten(), 0.75).item()\n",
    "        \n",
    "        # Sparsity measure (useful for detecting dead neurons)\n",
    "        sparsity = (weights == 0).float().mean().item()\n",
    "        \n",
    "        return f\"Episode {episode} | {title:<15}| Type: {layer_type:<6}| Shape: {list(weights.shape)}, Mean: {mean_val:.4f}, Std: {std_val:.4f}, Norm: {norm_val:.2f}, Range: [{min_val:.4f}, {max_val:.4f}], AbsMean: {abs_mean:.4f}, NearZero%: {near_zero_ratio:.2%}, Large%: {large_weight_ratio:.2%}, Q25/75: [{q25:.4f}, {q75:.4f}], Sparsity: {sparsity:.2%}\"\n",
    "\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def log_model_weights(self, model: torch.nn.Module, episode: int = -1, print_once: bool = False, include_bias: bool = True):\n",
    "        \"\"\"\n",
    "        Log weight statistics for all layers in the model\n",
    "        \n",
    "        Args:\n",
    "            model: PyTorch model to analyze\n",
    "            episode: Episode number\n",
    "            print_once: Whether to print once or always\n",
    "            include_bias: Whether to include bias terms in analysis\n",
    "        \"\"\"\n",
    "        orig_device = next(model.parameters()).device\n",
    "        model.cpu()\n",
    "        \n",
    "         # Get all modules ot the model except for activation functions        \n",
    "        all_modules = [name for name, _ in model.named_children() if name != \"phi\"]\n",
    "        \n",
    "        log_str = \"\"\n",
    "        \n",
    "        for module_name in all_modules:\n",
    "            model_module: torch.nn.Module = getattr(model, module_name)\n",
    "            for enc_idx, encoder in enumerate(model_module.modules_list):\n",
    "                for name, sub_module in encoder.named_modules():\n",
    "                    if len(list(sub_module.parameters())) > 0:  # Only modules with parameters\n",
    "                        for param_name, param in sub_module.named_parameters():\n",
    "                            if 'weight' in param_name or (include_bias and 'bias' in param_name):\n",
    "                                layer_type = type(sub_module).__name__.lower()\n",
    "                                full_name = f\"enc{enc_idx}_{name}_{param_name}\" if name else f\"enc{enc_idx}_{param_name}\"\n",
    "                                log_str += self.analyze_weights(param.data, episode, full_name, layer_type, print_once) + \"\\n\"\n",
    "        \n",
    "        \n",
    "        all_weights = torch.cat([p.data.flatten() for p in model.parameters() if p.requires_grad])\n",
    "        log_str += self.analyze_weights(all_weights, episode, \"ALL_WEIGHTS\", \"global\", print_once) + \"\\n\"\n",
    "        \n",
    "        log_str += f\"{'='*120}\\n\"\n",
    "        \n",
    "        # Log everything\n",
    "        self.log(log_str, print_once, improve_file_output=True)\n",
    "        \n",
    "        # Restore original device\n",
    "        model.to(orig_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torch Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "      \n",
    "class Parallel(nn.Module):\n",
    "    def __init__(self, modules: list[nn.Module]):\n",
    "        super().__init__()\n",
    "        if not isinstance(modules, list):\n",
    "            raise ValueError(\"Modules are not lists!\")\n",
    "        \n",
    "        self.model = nn.ModuleList(modules)\n",
    "        #self.model = nn.ModuleDict(modules) # Otherwise modules are not registered correctly\n",
    "\n",
    "    def forward(self, inputs: tuple) -> torch.Tensor:\n",
    "        all_logits = []\n",
    "        \n",
    "        for module, input in zip(self.model, inputs):\n",
    "            all_logits.append(module(input))\n",
    "            \n",
    "        return torch.cat(all_logits, dim=1)\n",
    "    \n",
    "class OwnModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def init_weights(self, bias: bool = True, debug: bool=False):  \n",
    "        \"\"\" Initialize weights for Linear features. Kaiming He for (Leaky-)Relu otherwise Xavier \"\"\"\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                if isinstance(self.phi, nn.ReLU):\n",
    "                    nn.init.kaiming_uniform_(module.weight, nonlinearity=\"relu\")\n",
    "                    print(\"Weights initialized with Kaiming He\") if debug else None\n",
    "                elif isinstance(self.phi, nn.LeakyReLU):\n",
    "                    nn.init.kaiming_uniform_(module.weight, nonlinearity=\"leaky_relu\")\n",
    "                    print(\"Weights initialized with Kaiming He\") if debug else None\n",
    "                else:\n",
    "                    nn.init.xavier_uniform_(module.weight) \n",
    "                    print(\"Weights initialized with Glorot\") if debug else None\n",
    "                    \n",
    "                if bias and module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "                    \n",
    "    def get_padding(self, kernel_size: int) -> int:\n",
    "        \"\"\" Preserve the image size if stride is one, otherwise reduce spacial dimension \"\"\"\n",
    "        return (kernel_size - 1) // 2\n",
    "        \n",
    "    def get_size_estimate(self) -> float:\n",
    "        param_size = 0\n",
    "        buffer_size = 0\n",
    "        for param in self.parameters():\n",
    "            param_size += param.nelement() * param.element_size()\n",
    "\n",
    "        for buffer in self.buffers():\n",
    "            buffer_size += buffer.nelement() * buffer.element_size()\n",
    "\n",
    "        size_all_mb = (param_size + buffer_size) / 1024**2 # Conversion from bytes to megabytes\n",
    "    \n",
    "        return size_all_mb\n",
    "    \n",
    "    def save_model(self, path: str = \"\", filename: str = \"model.pt\"):\n",
    "        \"\"\" Saves the model's state dict at the provided path in a subfolder based on the date. Name: \"model.pt\"\n",
    "\n",
    "        Args:\n",
    "            path (str, optional): Path where the model should be stored. Defaults to \"\".\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            dir_path = os.path.dirname(os.path.realpath(__file__)) if path == \"\" else path     \n",
    "            os.makedirs(dir_path, exist_ok=True)\n",
    "            \n",
    "            file_path = os.path.join(dir_path, filename)\n",
    "            \n",
    "            checkpoint = dict(\n",
    "                architecture= dict(\n",
    "                    input_dim = self.input_dim,\n",
    "                    action_space=self.action_space,\n",
    "                    obs_state_infos=self.obs_state_infos,\n",
    "                    feature_dim_cnns = self.feature_dim_cnns,\n",
    "                    hidden_dim_heads = self.hidden_dim_heads,\n",
    "                    phi = self.phi,\n",
    "                ),\n",
    "                state_dict = self.state_dict()\n",
    "            )\n",
    "            \n",
    "            torch.save(checkpoint, file_path)\n",
    "            \n",
    "            print(f\"Successfully stored the model: {file_path}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Failed to store the model: {e}\")\n",
    "            \n",
    "    \n",
    "        \n",
    "    \n",
    "    @classmethod  \n",
    "    def load_model(cls, path: str = \"\"):\n",
    "        \"\"\" Loads an instance of this model class from the provided path. \n",
    "\n",
    "        Args:\n",
    "            path (str, optional): Path to the state dicht. Defaults to \"\".\n",
    "\n",
    "        Returns:\n",
    "            PM_Model: An instance of the model class.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            checkpoint = torch.load(path, weights_only=False)\n",
    "                        \n",
    "            arch_params = checkpoint.get(\"architecture\")\n",
    "            state_dict = checkpoint.get(\"state_dict\")\n",
    "\n",
    "            if arch_params is None or state_dict is None:\n",
    "                print(f\"Failed to load model: Checkpoint file {path} is missing 'architecture' or 'state_dict'.\")\n",
    "                return None\n",
    "\n",
    "            loaded_model = cls(**arch_params)\n",
    "            loaded_model.load_state_dict(state_dict)\n",
    "            \n",
    "            return loaded_model\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load the model: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DQN Functions\n",
    "@torch.no_grad()\n",
    "def process_observation(obs: torch.Tensor, state_dims: dict = {}, device: str = \"cpu\", dtype = torch.float32, permute: bool = False) -> torch.Tensor|Dict[str, torch.Tensor]:\n",
    "    \"\"\"Process multi-buffer observation from environment, for training and for plotting. Returns a dictionary with the state as key and data as value. \"\"\"\n",
    "    \"\"\" NOTE: state dims is only for splitting it -> depreceated as model now does the transformation itself!\"\"\"\n",
    "    if isinstance(obs, torch.Tensor) and obs.size()[1] > 1:\n",
    "                \n",
    "        # Put state channel at the end for plotting (color detection)\n",
    "        if permute and obs.ndim == 3:\n",
    "            obs = obs.permute(1,2,0)\n",
    "            \n",
    "        elif permute and obs.ndim == 4:\n",
    "            obs = obs.permute(0,2,3,1)\n",
    "        \n",
    "        obs = obs.to(device, dtype=dtype)\n",
    "        \n",
    "        if state_dims:\n",
    "            state_dim_idx = obs.ndim - 1 if permute else obs.ndim - 3 # if permute last channel, regular case: channel 0 if 3 dimensions, otherwise channel 1 as 0 is batches.\n",
    "            obs_states = obs.split(list(state_dims.values()), dim=state_dim_idx)\n",
    "            return dict(zip(state_dims.keys(), obs_states))\n",
    "        \n",
    "        return obs\n",
    "        \n",
    "    else:\n",
    "        # Fallback: Single observation case\n",
    "        return {'screen': obs.to(device, dtype=dtype)}\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def epsilon_greedy(env, model: nn.Module, obs: list, epsilon: float, env_actions: EnvActions, device: str = \"cpu\", dtype = torch.float32, action_counter: ActionCounter = None, debug: bool = False):\n",
    "    \"\"\"Epsilon-greedy action selection for multi-buffer observations\"\"\"\n",
    "\n",
    "    num_players = env.num_players\n",
    "    \n",
    "    if random.random() < epsilon:\n",
    "       \n",
    "        # One different action (1-7) per player\n",
    "        chosen_actions = env_actions.get_random_action(n=num_players, use_proba=True)\n",
    "        print(\"WITHIN EPSILON:\", chosen_actions) if debug else None\n",
    "        \n",
    "    else:\n",
    "        model.eval()\n",
    "        \n",
    "        if not isinstance(obs, torch.Tensor):\n",
    "            obs = torch.stack(obs) # stack players\n",
    "        \n",
    "        processed_obs = process_observation(obs, device=device, dtype=dtype, permute=False)\n",
    "        q_values = model(processed_obs)\n",
    "        \n",
    "        # Indices are necessary if buttons don't start at 0 but at 1 (e.g., if there wouldn't be a noop option)\n",
    "    \n",
    "        if num_players == 1:\n",
    "            chosen_actions_idx = q_values.argmax().to(dtype=int).item() # Single action\n",
    "            print(\"DEBUG EPSILON NUM PLAYERS=1:\", chosen_actions_idx, q_values, end=\"\")  if debug else None\n",
    "        else:\n",
    "            chosen_actions_idx = q_values.argmax(dim=1).to(dtype=int).tolist() # list of actions\n",
    "            print(\"DEBUG EPSILON NUM PLAYERS>1:\", chosen_actions_idx, q_values, end=\"\")  if debug else None\n",
    "        \n",
    "        chosen_actions = env_actions.get_action_value(chosen_actions_idx) \n",
    "        print(chosen_actions) if debug else None\n",
    "        \n",
    "    # Add to action counter\n",
    "    if action_counter:\n",
    "        if isinstance(chosen_actions, list):\n",
    "            for action in chosen_actions:\n",
    "                action_counter.add(action)\n",
    "        else: # single action\n",
    "            action_counter.add(chosen_actions)\n",
    "\n",
    "    return chosen_actions\n",
    "\n",
    "def hard_update_target_network(target_net: nn.Module, main_net: nn.Module):\n",
    "    \"\"\"Hard update of target network\"\"\"\n",
    "    target_net.load_state_dict(main_net.state_dict())\n",
    "    \n",
    "    \n",
    "    # Update Target network\n",
    "def soft_update_target_network(local_model: nn.Module, target_model: nn.Module, tau: float):\n",
    "    \"\"\"Soft update model parameters.\n",
    "    θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "    Params\n",
    "    ======\n",
    "        local_model (PyTorch model): weights will be copied from\n",
    "        target_model (PyTorch model): weights will be copied to\n",
    "        tau (float): interpolation parameter\n",
    "    \"\"\"\n",
    "    # TODO: Update target network\n",
    "    \n",
    "    # Store current state dicts\n",
    "    local_state_dict = local_model.state_dict()\n",
    "    target_state_dict = target_model.state_dict()\n",
    "    \n",
    "    # Iterate over the modules and soft update the target state dict with parameter tau\n",
    "    for module in local_state_dict.keys():\n",
    "        target_state_dict[module] = local_state_dict[module] * tau + target_state_dict[module] * (1 - tau)\n",
    "    \n",
    "    # Load the soft updated values\n",
    "    target_model.load_state_dict(target_state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientDQN(OwnModule):\n",
    "    \"\"\"\n",
    "    EfficientDQN with multi-buffer visual processing\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim: int, action_space: int, obs_state_infos: ExtraStates, feature_dim_cnns: int = 256, hidden_dim_heads: int = 1024, phi: nn.Module = nn.LeakyReLU(), dropout_p:float = 0.15):\n",
    "        #obs_state_infos: ExtraStates = None, \n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim # not used\n",
    "        self.obs_state_infos = obs_state_infos\n",
    "        self.action_space = action_space\n",
    "        self.hidden_dim_heads = hidden_dim_heads\n",
    "        self.phi = phi\n",
    "        self.dropout_p = dropout_p\n",
    "        \n",
    "        # Feature dimension after encoding\n",
    "        self.feature_dim_cnns = feature_dim_cnns\n",
    "\n",
    "        # Observation states info\n",
    "        self.obs_state_dims = obs_state_infos.get_dims(return_dict=False) # Indices to split the observation\n",
    "        self.obs_states_num = obs_state_infos.num_states\n",
    "        \n",
    "        # Separate encoders for each buffer type\n",
    "        self.encoders = Parallel([self._build_encoder(dim) for dim in self.obs_state_dims])\n",
    "        \n",
    "        \n",
    "        self.head_first = nn.Sequential(\n",
    "            #nn.Linear(self.feature_dim_cnns * self.obs_states_num, self.hidden_dim_heads),\n",
    "            nn.Linear(np.array([2*2*4 * [32 if dim == 3 else 16 for dim in self.obs_state_dims]]).sum(), self.hidden_dim_heads),\n",
    "            self.phi,\n",
    "            nn.Dropout(self.dropout_p),\n",
    "            nn.BatchNorm1d(self.hidden_dim_heads),\n",
    "        )\n",
    "        \n",
    "        # Dueling network heads\n",
    "        self.value_head = nn.Sequential(\n",
    "            nn.Linear(self.hidden_dim_heads, self.hidden_dim_heads // 8),\n",
    "            self.phi,\n",
    "            nn.Dropout(self.dropout_p),\n",
    "            nn.BatchNorm1d(self.hidden_dim_heads // 8),\n",
    "            nn.Linear(self.hidden_dim_heads // 8, 1)\n",
    "        )\n",
    "        \n",
    "        self.advantage_head = nn.Sequential(\n",
    "            nn.Linear(self.hidden_dim_heads, self.hidden_dim_heads // 8),\n",
    "            self.phi,\n",
    "            nn.Dropout(self.dropout_p),\n",
    "            nn.BatchNorm1d(self.hidden_dim_heads // 8),\n",
    "            nn.Linear(self.hidden_dim_heads // 8, action_space) \n",
    "        )\n",
    "        \n",
    "        params_encoder = sum(p.numel() for p in self.encoders.parameters() if p.requires_grad)\n",
    "        params_head_first = sum(p.numel() for p in self.head_first.parameters() if p.requires_grad)\n",
    "        params_advantage_head = sum(p.numel() for p in self.advantage_head.parameters() if p.requires_grad)\n",
    "        params_value_head = sum(p.numel() for p in self.value_head.parameters() if p.requires_grad)\n",
    "                \n",
    "        self.init_weights(bias=False, debug=True)\n",
    "        \n",
    "        print(f\"Initialized model with {params_encoder  + params_head_first + params_advantage_head + params_value_head} parameters!\")\n",
    "    \n",
    "    def _build_encoder(self, input_channels: int) -> nn.Module:\n",
    "        \"\"\"Build CNN encoder with residual blocks for visual input\"\"\"\n",
    "        \n",
    "        first_channel_out = 32 if input_channels == 3 else 16\n",
    "        \n",
    "        return nn.Sequential(\n",
    "            # Initial convolution - reduce spatial dimensions significantly\n",
    "            nn.Conv2d(input_channels, first_channel_out, 8, stride=4, padding=2),\n",
    "            self.phi,  # Output [N, C=16/4, WH=32\n",
    "            nn.BatchNorm2d(first_channel_out),\n",
    "\n",
    "            # Second CNN stage - double channels\n",
    "            nn.Conv2d(first_channel_out, first_channel_out * 2, 4, stride=3, padding=0), # [N, 2C, 10]\n",
    "            self.phi,\n",
    "            nn.BatchNorm2d(first_channel_out * 2),\n",
    "            \n",
    "            # Third residual stage - double channels again, with kernel\n",
    "            nn.Conv2d(first_channel_out * 2, first_channel_out * 4, 4, stride=2, padding=0), # [N, 4C, 4]\n",
    "            self.phi,\n",
    "            nn.BatchNorm2d(first_channel_out * 4),\n",
    "\n",
    "            # Fourth residual stage - double channels again, with kernel\n",
    "            nn.Conv2d(first_channel_out * 4, first_channel_out * 4, 2, stride=2), # [N, 4C, 4]\n",
    "            self.phi,\n",
    "            nn.BatchNorm2d(first_channel_out * 4),\n",
    "        \n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "    \n",
    "    def forward(self, observations: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through multi-buffer DQN\n",
    "        \n",
    "        Args:\n",
    "            observations: Tensor for \"screen\", \"depth\", \"labels\", \"automap\"\n",
    "        \"\"\"                \n",
    "        # Encode each visual buffer and split the observations\n",
    "        features = self.encoders(observations.split(self.obs_state_dims, dim=1))\n",
    "        #print(\"Features:\", features.shape)\n",
    "\n",
    "        # Use one head for the dueling network                \n",
    "        head_logits = self.head_first(features)\n",
    "        #print(\"Features:\", head_logits.shape)\n",
    "\n",
    "        # Dueling network computation\n",
    "        value = self.value_head(head_logits)\n",
    "        #print(\"Value:\", value.shape)\n",
    "\n",
    "        advantage = self.advantage_head(head_logits)\n",
    "        #print(\"Advantage:\", advantage.shape)\n",
    "\n",
    "        # Combine value and advantage\n",
    "        q_values = value + advantage - advantage.mean(dim=1, keepdim=True)\n",
    "        \n",
    "        return q_values\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reward_components(reward_log, smooth_window: int = 5):\n",
    "    \"\"\"\n",
    "    Plot raw and smoothed episode-level reward components.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    reward_log : list[dict]\n",
    "        Append a dict for each episode, e.g. {\"frag\": …, \"hit\": …, \"hittaken\": …}\n",
    "    smooth_window : int\n",
    "        Rolling-mean window size for the smoothed curve.\n",
    "    \"\"\"\n",
    "    if not reward_log:\n",
    "        print(\"reward_log is empty – nothing to plot.\")\n",
    "        return\n",
    "\n",
    "    df = pd.DataFrame(reward_log)\n",
    "    df_smooth = df.rolling(window=smooth_window, min_periods=1).mean()\n",
    "\n",
    "    # raw\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    for col in df.columns:\n",
    "        plt.plot(df.index, df[col], label=col)\n",
    "    plt.title(\"Raw episode reward components\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # smoothed\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    for col in df.columns:\n",
    "        plt.plot(df.index, df_smooth[col], label=f\"{col} (avg)\")\n",
    "    plt.title(f\"Smoothed (window={smooth_window})\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def plot_training_metrics(reward_history, loss_history, epsilon_history):\n",
    "    \"\"\"Plot training metrics\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Reward history\n",
    "    axes[0, 0].plot(reward_history)\n",
    "    axes[0, 0].plot(pd.Series(reward_history).rolling(50).mean(), \"r-\", alpha=0.7)\n",
    "    axes[0, 0].set_title(\"Episode Rewards\")\n",
    "    axes[0, 0].set_xlabel(\"Episode\")\n",
    "    axes[0, 0].set_ylabel(\"Reward\")\n",
    "    axes[0, 0].grid(True)\n",
    "    \n",
    "    # Loss history\n",
    "    if loss_history:\n",
    "        axes[0, 1].plot(loss_history)\n",
    "        axes[0, 1].plot(pd.Series(loss_history).rolling(100).mean(), \"r-\", alpha=0.7)\n",
    "        axes[0, 1].set_title(\"Training Loss\")\n",
    "        axes[0, 1].set_xlabel(\"Training Step\")\n",
    "        axes[0, 1].set_ylabel(\"Loss\")\n",
    "        axes[0, 1].grid(True)\n",
    "    \n",
    "    # Epsilon decay\n",
    "    axes[1, 0].plot(epsilon_history)\n",
    "    axes[1, 0].set_title(\"Epsilon Decay\")\n",
    "    axes[1, 0].set_xlabel(\"Episode\")\n",
    "    axes[1, 0].set_ylabel(\"Epsilon\")\n",
    "    axes[1, 0].grid(True)\n",
    "    \n",
    "    # Reward distribution\n",
    "    axes[1, 1].hist(reward_history, bins=50, alpha=0.7)\n",
    "    axes[1, 1].set_title(\"Reward Distribution\")\n",
    "    axes[1, 1].set_xlabel(\"Reward\")\n",
    "    axes[1, 1].set_ylabel(\"Frequency\")\n",
    "    axes[1, 1].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def plot_images(obs: torch.Tensor, state_dims: dict, **kwargs):\n",
    "    \"\"\" Plots each state of the observations in one plot. \"\"\"\n",
    "    \n",
    "    num_plots = len(state_dims)\n",
    "    \n",
    "    obs_processed = process_observation(obs, state_dims, device=\"cpu\", permute=True)\n",
    "    \n",
    "    last_plot = 1\n",
    "    \n",
    "    if kwargs:\n",
    "        plt.suptitle(kwargs.get(\"title\", \"\"))\n",
    "    \n",
    "    def add_plot(data, last_plot, title):\n",
    "        plt.subplot(100 + 10*num_plots + last_plot)\n",
    "        plt.imshow(data)\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(title)\n",
    "        return last_plot + 1\n",
    "\n",
    "    for key in state_dims.keys():\n",
    "        last_plot = add_plot(obs_processed.get(key), last_plot, key)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def replay_episode(env, model, device, dtype, path: str = \"\", store: bool = False, random_player: bool = True):\n",
    "    # ----------------------------------------------------------------\n",
    "    # Hint for replay visualisation:\n",
    "    # ----------------------------------------------------------------\n",
    "\n",
    "    env.enable_replay()\n",
    "\n",
    "    # Tracking reward components\n",
    "    eval_reward = 0.0\n",
    "\n",
    "    # Reset environment\n",
    "    with suppress_output():\n",
    "        eval_obs = env.reset()\n",
    "    \n",
    "    eval_dones = [False]\n",
    "    model.cpu()\n",
    "\n",
    "    while not all(eval_dones):\n",
    "        env_actions = EnvActions(env)\n",
    "        \n",
    "        eval_act = epsilon_greedy(env, model, eval_obs, 0, env_actions, \"cpu\", dtype=dtype)\n",
    "        eval_obs, reward_components, eval_dones, _ = env.step(eval_act)\n",
    "        eval_reward += sum(reward_components)\n",
    "\n",
    "    print(f\"Final evaluation - Total reward: {eval_reward:.1f}\")\n",
    "\n",
    "    # Finalize episode\n",
    "    env.disable_replay()\n",
    "\n",
    "    replays = env.get_player_replays()\n",
    "\n",
    "    if random_player:\n",
    "        # Random Player\n",
    "        player_idx = np.random.randint(0, len(replays))\n",
    "        player_name = list(replays.keys())[player_idx]\n",
    "        replays = {player_name: replays.get(player_name)}\n",
    "    \n",
    "    if store:\n",
    "        path = os.path.join(path, f\"{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}_{eval_reward:.0f}.mp4\")\n",
    "        render_episode(replays, subsample=5, replay_path=path)\n",
    "    else:\n",
    "        HTML(render_episode(replays, subsample=5).to_html5_video())\n",
    "\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    \n",
    "    \n",
    "def get_average_result(episode_metrics: Dict[int, dict]) -> dict:\n",
    "    \"\"\" Returns the average reward from all players for each reward type. \"\"\"\n",
    "    averaged_metrics = dict()\n",
    "\n",
    "    for values in episode_metrics.values():\n",
    "        for key, reward_val in values.items():\n",
    "            if isinstance(averaged_metrics.get(key), list):\n",
    "                averaged_metrics[key].append(reward_val)\n",
    "            else:\n",
    "                averaged_metrics[key] = [reward_val]\n",
    "                \n",
    "    return {key: np.mean(value, dtype=int) for key, value in averaged_metrics.items()}\n",
    "        \n",
    "\n",
    "def get_avg_reward(reward_history: dict, episodes: int = 1, player_idx: int=-1, round:int = 2) -> np.ndarray:\n",
    "    \"\"\" Returns the average reward for the episode or player id\n",
    "\n",
    "    Args:\n",
    "        reward_history (dict): A dictionary with player id as key and the rewards as values\n",
    "        episodes (int, optional): Number of last episode to extract. Defaults to 1 (=last).\n",
    "        player_idx (int, optional): Player idx to filter. Defaults to -1.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Average rewards as an array\n",
    "    \"\"\"\n",
    "    df_rwds = pd.DataFrame.from_dict(reward_history)\n",
    "    \n",
    "    if player_idx == -1:\n",
    "        df_rwds = df_rwds.mean(axis=1) # Mean over columns\n",
    "        \n",
    "    else:\n",
    "        df_rwds = df_rwds.iloc[:, player_idx]\n",
    "        \n",
    "    if episodes > 1:\n",
    "        df_rwds = df_rwds[-episodes:].mean(axis=0) # Mean over columns\n",
    "    else:\n",
    "        df_rwds = df_rwds.iloc[episodes]\n",
    "        \n",
    "    return np.round(df_rwds, round)\n",
    "        \n",
    "\n",
    "@contextlib.contextmanager\n",
    "def suppress_output():\n",
    "    \"\"\"Suppress both stdout and stderr, including output from C extensions.\"\"\"\n",
    "    with open(os.devnull, 'w') as devnull:\n",
    "        # Save original file descriptors\n",
    "        old_stdout_fd = os.dup(1)\n",
    "        old_stderr_fd = os.dup(2)\n",
    "\n",
    "        try:\n",
    "            # Redirect stdout and stderr to devnull\n",
    "            os.dup2(devnull.fileno(), 1)\n",
    "            os.dup2(devnull.fileno(), 2)\n",
    "\n",
    "            # Also suppress Python-level stdout/stderr\n",
    "            with contextlib.redirect_stdout(devnull), contextlib.redirect_stderr(devnull):\n",
    "                yield\n",
    "        finally:\n",
    "            # Restore original file descriptors\n",
    "            os.dup2(old_stdout_fd, 1)\n",
    "            os.dup2(old_stderr_fd, 2)\n",
    "            os.close(old_stdout_fd)\n",
    "            os.close(old_stderr_fd)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pDz9CHjVzP61"
   },
   "source": [
    "## Environment configuration\n",
    "\n",
    "ViZDoom supports multiple visual buffers that can be used as input for training agents. Each buffer provides different information about the game environment, as seen from left to right:\n",
    "\n",
    "\n",
    "Screen\n",
    "- The default first-person RGB view seen by the agent.\n",
    "\n",
    "Labels\n",
    "- A semantic map where each pixel is tagged with an object ID (e.g., enemy, item, wall).\n",
    "\n",
    "Depth\n",
    "- A grayscale map showing the distance from the agent to surfaces in the scene.\n",
    "\n",
    "Automap\n",
    "- A top-down schematic view of the map, useful for global navigation tasks.\n",
    "\n",
    "![buffers gif](https://vizdoom.farama.org/_images/vizdoom-demo.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folder for training\n",
    "training_id = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "training_folder = os.path.join(\"runs_test\", training_id)\n",
    "\n",
    "os.makedirs(training_folder, exist_ok=True)\n",
    "print(\"Using folder:\", training_folder)\n",
    "\n",
    "# Define loggers\n",
    "logger = Logger(training_folder, also_print=True)\n",
    "activation_logger = ActivationLogger(training_folder, filename=\"activations.txt\", also_print=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nmNDlnmfzP62"
   },
   "outputs": [],
   "source": [
    "USE_GRAYSCALE = False  # ← flip to False for RGB\n",
    "\n",
    "PLAYER_CONFIG = {\n",
    "    # NOTE: \"algo_type\" defaults to POLICY in evaluation script!\n",
    "    \"algo_type\": \"QVALUE\",  # OPTIONAL, change to POLICY if using policy-based (eg PPO)\n",
    "    \"n_stack_frames\": 1, #4, # 1 # Temporal information\n",
    "    \"extra_state\": [\"depth\"],#, \"labels\"], #[\"labels\"],#, \"automap\"], #[\"depth\", \"labels\", \"automap\"],\n",
    "    \"hud\": \"none\",\n",
    "    \"crosshair\": True,\n",
    "    \"screen_format\": 8 if USE_GRAYSCALE else 0,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A3vdawvQzP62"
   },
   "outputs": [],
   "source": [
    "# TODO: environment training paramters\n",
    "N_STACK_FRAMES = 1 # 4 does not work yet!\n",
    "NUM_PLAYERS = 4 #3\n",
    "NUM_BOTS = 6 #1, #6\n",
    "EPISODE_TIMEOUT = 2000\n",
    "# TODO: model hyperparams\n",
    "GAMMA = 0.95\n",
    "EPISODES = 2 #2 # 500\n",
    "BATCH_SIZE = 128\n",
    "REPLAY_BUFFER_SIZE = 15_000\n",
    "LEARNING_RATE = 1e-5\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_END = 0.1\n",
    "EPSILON_DECAY = 0.998 # 0.987\n",
    "N_EPOCHS = 20\n",
    "FRAME_SKIPPING = 0\n",
    "FRAME_SKIPPING_STOP = 100\n",
    "DEBUG = True\n",
    "PRINT_EVERY = 50\n",
    "VIDEO_DURING_TRAINING = False\n",
    "EVALUATION_EVERY = 100\n",
    "CHECKPOINT_EVERY = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OzTS4m2VzP63",
    "outputId": "369317da-3265-4f5c-b2f1-7990ca7d1f44"
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)\n",
    "\n",
    "DTYPE = torch.float32\n",
    "\n",
    "obs_states = ExtraStates(PLAYER_CONFIG[\"extra_state\"] + [\"screen\"], num_frames = N_STACK_FRAMES)\n",
    "\n",
    "reward_fn = YourReward(num_players=NUM_PLAYERS)\n",
    "\n",
    "def build_env():\n",
    "    env = VizdoomMPEnv(\n",
    "        num_players=NUM_PLAYERS, # 4 is max\n",
    "        num_bots=NUM_BOTS,\n",
    "        bot_skill=1, # Increased difficulty\n",
    "        doom_map=\"TRNM\",  # ROOM = NOTE simple, small map; other options: TRNM, TRNMBIG\n",
    "        extra_state=PLAYER_CONFIG[\"extra_state\"],  # see info about states at the beginning of \"Environment configuration\" above\n",
    "        episode_timeout=EPISODE_TIMEOUT,\n",
    "        n_stack_frames=PLAYER_CONFIG[\"n_stack_frames\"],\n",
    "        crosshair=PLAYER_CONFIG[\"crosshair\"],\n",
    "        hud=PLAYER_CONFIG[\"hud\"],\n",
    "        screen_format=PLAYER_CONFIG[\"screen_format\"],\n",
    "        reward_fn=reward_fn,\n",
    "        seed = rng.integers(1, 2000, NUM_PLAYERS),\n",
    "        ticrate = 35#*3,\n",
    "    )\n",
    "    return env\n",
    "\n",
    "env = build_env()\n",
    "with suppress_output(): # Reset to get player and game information\n",
    "    _ = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KkuIefaSzP64"
   },
   "source": [
    "## Agent\n",
    "\n",
    "Implement **your own agent** in the code cell that follows.\n",
    "\n",
    "* In `agents/dqn.py` and `agents/ppo.py` you’ll find very small **skeletons**—they compile but are meant only as reference or quick tests.  \n",
    "  Feel free to open them, borrow ideas, extend them, or ignore them entirely.\n",
    "* The notebook does **not** import those files automatically; whatever class you define in the next cell is the one that will be trained.\n",
    "* You may keep the DQN interface, switch to PPO, or try something else.\n",
    "* Tweak any hyper-parameters (`PLAYER_CONFIG`, ε-schedule, optimiser, etc.) and document what you tried.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n7dWjwTdzP65"
   },
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# Initialise your networks and training utilities\n",
    "# ================================================================\n",
    "env_actions = EnvActions(env, rng=rng)\n",
    "\n",
    "# main Q-network\n",
    "model = EfficientDQN(\n",
    "    input_dim = 0,\n",
    "    action_space=env_actions.action_space,\n",
    "    obs_state_infos=obs_states,\n",
    ").to(device, dtype=DTYPE)\n",
    "\n",
    "# Load a model from file\n",
    "#model = model.load_model(\"runs/20250621_182219/best_model_20250621_182229.pt\").to(device, dtype=DTYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_model  = deepcopy(model).to(device, dtype=DTYPE)\n",
    "optimizer  = torch.optim.AdamW(target_model.parameters(), lr=LEARNING_RATE, weight_decay=5e-4)\n",
    "scheduler  = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=50, eta_min=1e-8)\n",
    "loss_fn = nn.MSELoss()\n",
    "replay_buffer = deque(maxlen=REPLAY_BUFFER_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset environment\n",
    "with suppress_output():\n",
    "    obs_test: list = env.reset()\n",
    "    \n",
    "for player in range(len(obs_test)):\n",
    "    plot_images(obs_test[player], obs_states.get_dims())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate 60 steps\n",
    "STEPS = 30\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(STEPS):\n",
    "        actions = env_actions.get_random_action(NUM_PLAYERS)\n",
    "        #actions = env_actions.get_action_value(1, NUM_PLAYERS)\n",
    "        #actions = epsilon_greedy(env, model, obs_test, 0.5, env_actions, device, DTYPE, debug=True) # 50% random actions by model\n",
    "        actions = epsilon_greedy(env, model, torch.rand_like(torch.stack(obs_test)), 0, env_actions, device, DTYPE, debug=False) # model fully random\n",
    "        obs_test, reward = env.step(actions)[0:2]\n",
    "\n",
    "    for player in range(env.num_players):\n",
    "        action_print = [actions] if isinstance(actions, int) else actions\n",
    "        plot_images(obs_test[player], obs_states.get_dims(), **{\"title\": f\"Reward: {reward[player]:.1f} | Action: {env_actions.get_action_name(action_print[player])}\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of Reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_idx = rng.integers(env.num_players)\n",
    "\n",
    "player_game_vars = env.envs[player_idx].unwrapped._game_vars\n",
    "player_game_vars_pre = env.envs[player_idx].unwrapped._game_vars_pre\n",
    "reward = reward_fn(0, player_game_vars, player_game_vars_pre, 0)\n",
    "print(f\"Reward: {np.sum(reward)} | {reward}\")\n",
    "for key, value_now, value_old in zip(player_game_vars, player_game_vars.values(), player_game_vars_pre.values()):   \n",
    "    print(f\"{key+':':<25}{value_now:.0f} | {value_old:.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mihj8Q5xzP65"
   },
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_model_tensors(episode: int, obs_clean: tuple[torch.Tensor], model: nn.Module, model_sequence: list = [None, 0, 1, 1], print_once: bool = False):    \n",
    "    rand_tensor = torch.rand(size=torch.stack(obs_clean).shape).to(\"cpu\").split(obs_states.get_dims(return_dict=False), dim=1)\n",
    "    \n",
    "    value, advantage = activation_logger.log_model_activations(rand_tensor, model, model_sequence, episode=episode, return_activations_from_idx=-2, print_once=print_once)\n",
    "    q_values = (value + advantage - advantage.mean(dim=1, keepdim=True)).to(dtype=DTYPE)\n",
    "    q_actions = q_values.argmax(dim=1).to(dtype=DTYPE)\n",
    "    \n",
    "    log_qvals = activation_logger.analyze_activations(q_values, episode, title=\"Qvalues\", print_once=print_once)\n",
    "    log_qacts = activation_logger.analyze_activations(q_actions, episode, title=\"Actions\", print_once=print_once)\n",
    "    activation_logger.log(log_qvals + \"\\n\" + log_qacts, improve_file_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------  TRAINING LOOP  ----------------------\n",
    "training_info_str = (\n",
    "    f\"N_STACK_FRAMES: {N_STACK_FRAMES}, \"\n",
    "    f\"NUM_PLAYERS: {NUM_PLAYERS}, \"\n",
    "    f\"NUM_BOTS: {NUM_BOTS}, \"\n",
    "    f\"EPISODE_TIMEOUT: {EPISODE_TIMEOUT}, \"\n",
    "    f\"GAMMA: {GAMMA}, \"\n",
    "    f\"EPISODES: {EPISODES}, \"\n",
    "    f\"BATCH_SIZE: {BATCH_SIZE}, \"\n",
    "    f\"REPLAY_BUFFER_SIZE: {REPLAY_BUFFER_SIZE}, \"\n",
    "    f\"LEARNING_RATE: {LEARNING_RATE}, \"\n",
    "    f\"EPSILON_START: {EPSILON_START}, \"\n",
    "    f\"EPSILON_END: {EPSILON_END}, \"\n",
    "    f\"EPSILON_DECAY: {EPSILON_DECAY}, \"\n",
    "    f\"N_EPOCHS: {N_EPOCHS}, \"\n",
    "    f\"FRAME_SKIPPING: {FRAME_SKIPPING}, \"\n",
    "    f\"FRAME_SKIPPING_STOP: {FRAME_SKIPPING_STOP}, \"\n",
    "    f\"DEBUG: {DEBUG}, \"\n",
    "    f\"PRINT_EVERY: {PRINT_EVERY}, \"\n",
    "    f\"VIDEO_DURING_TRAINING: {VIDEO_DURING_TRAINING}, \"\n",
    "    f\"EVALUATION_EVERY: {EVALUATION_EVERY}\"\n",
    ")\n",
    "\n",
    "training_info_str += ', '.join([f\"{k}: {v}\" for k, v in PLAYER_CONFIG.items()])\n",
    "\n",
    "logger.log(training_info_str, improve_file_output=True)\n",
    "\n",
    "# Training settings\n",
    "q_loss_list, epsilon_history = [], []\n",
    "reward_history = {player: [] for player in range(env.num_players)}\n",
    "best_reward = float(\"-inf\")\n",
    "best_model = None\n",
    "\n",
    "epsilon = EPSILON_START \n",
    "steps_done = 0\n",
    "\n",
    "\n",
    "with TqdmProgress(total=EPISODES) as progress_bar:\n",
    "    for episode in progress_bar:\n",
    "        \n",
    "        episode_action_counter = ActionCounter()\n",
    "        progress_bar.set_description(episode)\n",
    "        \n",
    "        with suppress_output():\n",
    "            #obs = torch.stack([env.reset() for env in environments.values()])\n",
    "            obs: torch.Tensor = env.reset()\n",
    "        \n",
    "        # Episode variables for each player\n",
    "        episode_metrics = {player: {\"frags\": 0, \"hits\": 0, \"damage_taken\": 0, \"movement\": 0, \"ammo_efficiency\": 0, \"survival\": 0, \"health_pickup\": 0} for player in range(env.num_players)}\n",
    "        episode_reward = {player: 0.0 for player in range(env.num_players)}\n",
    "\n",
    "        dones = [False]\n",
    "        model.eval()\n",
    "        \n",
    "        episode_msg = f\"{datetime.now().strftime('%H:%M:%S')} | Episode: {episode} | Steps done: {steps_done} | Gathering rollout (currently {len(replay_buffer)})\"\n",
    "        logger.log(episode_msg)\n",
    "        \n",
    "        # ───────── rollout ─────────────────────────────────────────────\n",
    "        while not all(dones):\n",
    "            \n",
    "            # Skip Frames (Testing - I am aware that in the original paper they stacked the images. I want to get many more steps at the beginning)\n",
    "            act = epsilon_greedy(env, model, obs, epsilon, env_actions, device, DTYPE, episode_action_counter)\n",
    "            \n",
    "            max_skip_steps = max(FRAME_SKIPPING * (episode < FRAME_SKIPPING_STOP), 1)\n",
    "            for skip_steps in range(max_skip_steps):\n",
    "                if all(dones):\n",
    "                    break\n",
    "                next_obs, reward, dones, _ = env.step(act)\n",
    "                steps_done += 1\n",
    "\n",
    "            # ----- reward definition ----------------\n",
    "            for player_idx in range(env.num_players):\n",
    "                episode_reward[player_idx] += np.sum(reward[player_idx]) # Should be a list with one value\n",
    "                \n",
    "                if dones[player_idx]:\n",
    "                    break\n",
    "                \n",
    "                reward_components = reward_fn(None, env.envs[player_idx].unwrapped._game_vars, env.envs[player_idx].unwrapped._game_vars_pre, None)\n",
    "\n",
    "                if len(reward_components) >= 6:\n",
    "                    episode_metrics[player_idx][\"frags\"] += reward_components[0]\n",
    "                    episode_metrics[player_idx][\"hits\"] += reward_components[1]\n",
    "                    episode_metrics[player_idx][\"damage_taken\"] += reward_components[2]\n",
    "                    episode_metrics[player_idx][\"movement\"] += reward_components[3]\n",
    "                    episode_metrics[player_idx][\"ammo_efficiency\"] += reward_components[4]\n",
    "                    episode_metrics[player_idx][\"survival\"] += reward_components[5]\n",
    "                    episode_metrics[player_idx][\"health_pickup\"] += reward_components[6]\n",
    "            \n",
    "                # ----- buffer and environment handling ----------------\n",
    "                if env.num_players == 1:\n",
    "                    act = [act]\n",
    "                    \n",
    "                replay_buffer.append((obs[player_idx], act[player_idx], reward[player_idx], next_obs[player_idx], dones[player_idx]))\n",
    "\n",
    "            obs = next_obs\n",
    "            \n",
    "        [reward_history[player_idx].append(reward) for player_idx, reward in episode_reward.items()] # Append rewards for every player\n",
    "        epsilon_history.append(epsilon)\n",
    "\n",
    "        # ───────── learning step (experience replay) ──────────────────\n",
    "        if len(replay_buffer) >= BATCH_SIZE:\n",
    "\n",
    "            model.train()\n",
    "            train_msg = f\"{datetime.now().strftime('%H:%M:%S')} | Episode: {episode} | Training for {N_EPOCHS} epochs\"\n",
    "            logger.log(train_msg)\n",
    "            \n",
    "            for epoch in range(N_EPOCHS):\n",
    "\n",
    "                batch = random.sample(replay_buffer, BATCH_SIZE)\n",
    "                states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "                # Convert to tensors for training\n",
    "                states = torch.stack(states).to(device, DTYPE)\n",
    "                next_states = torch.stack(next_states).to(device, DTYPE)\n",
    "                actions = torch.tensor(actions, device=device)\n",
    "                rewards = torch.tensor(rewards, device=device, dtype=torch.float32) #.squeeze(1) # Added squeeze here\n",
    "                dones = torch.tensor(dones, device=device, dtype=torch.float32)\n",
    "\n",
    "                current_q = model(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "                \n",
    "                # Target q values\n",
    "                with torch.no_grad():\n",
    "                    next_q = target_model(next_states).max(1)[0] #.values\n",
    "                    target_q = torch.clamp(rewards + GAMMA * next_q * (1 - dones), min=-10, max=10) # clamping to staibilize training\n",
    "                \n",
    "                loss = loss_fn(current_q, target_q) # TODO: Check whether correct, did that in Deep Q Assignment\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                \n",
    "                q_loss_list.append(loss.item())\n",
    "            \n",
    "            # Update scheduler and epsilon\n",
    "            scheduler.step()\n",
    "            epsilon = max(EPSILON_END, epsilon * EPSILON_DECAY)\n",
    "            \n",
    "            progress_bar.update_step_count()\n",
    "\n",
    "            # Update target network        \n",
    "            target_update_msg = f\"{datetime.now().strftime('%H:%M:%S')} | Episode: {episode} | Updating target network...\"\n",
    "            logger.log(target_update_msg)\n",
    "            \n",
    "            soft_update_target_network(model, target_model, tau=1e-3)\n",
    "            #hard_update_target_network(target_model, model)\n",
    "            \n",
    "        else:\n",
    "            train_fail_msg = f\"{datetime.now().strftime('%H:%M:%S')} | Episode: {episode} | Replay buffer smaller than batchsize {len(replay_buffer)} {BATCH_SIZE}\"\n",
    "            logger.log(train_fail_msg, print_once=True)\n",
    "\n",
    "        # -------- logging ----------\n",
    "        avg_reward = get_avg_reward(reward_history, episodes=PRINT_EVERY, round=0)\n",
    "        avg_loss = np.mean(q_loss_list[-10:]) if q_loss_list else 0\n",
    "        stacked_episode_reward = np.array([np.round(reward, 0) for reward in episode_reward.values()])\n",
    "        \n",
    "        action_counts = episode_action_counter.get_counts()\n",
    "\n",
    "        # Rewards and losses\n",
    "        reward_msg = f\"{datetime.now().strftime('%H:%M:%S')} | Episode: {episode} | Rewards:\\n\"\n",
    "        reward_msg += f\"\\tReward: {stacked_episode_reward} | Avg Reward: {avg_reward} | Loss: {avg_loss:.4f} | ε: {epsilon:.3f} | LR: {scheduler.get_last_lr()[0]:.2e}\"\n",
    "        reward_msg += f\"\\n\\tMetrics - {[f'{metric}: {value}' for metric, value in get_average_result(episode_metrics).items()]}\"\n",
    "        reward_msg += f\"\\n\\tActions - {', '.join([f'{k}: {v}' for k, v in episode_action_counter.get_name_counts(env_actions).items()])}\"\n",
    "\n",
    "        logger.log(reward_msg, print_once = episode % PRINT_EVERY == 0)\n",
    "        \n",
    "        progress_bar.set_postfix({\n",
    "            \"Avg Reward\": f\"{avg_reward}\",\n",
    "            \"Loss\": f\"{avg_loss:.4f}\",\n",
    "            \"Epsilon\": f\"{epsilon:.2f}\"\n",
    "        })\n",
    "            \n",
    "        # Show the video \n",
    "        if VIDEO_DURING_TRAINING and episode % EVALUATION_EVERY == 0:\n",
    "            replay_msg = f\"{datetime.now().strftime('%H:%M:%S')} | Episode: {episode} | Replaying animation...\"\n",
    "            logger.log(replay_msg)\n",
    "            with suppress_output():\n",
    "                replay_episode(env, model, device, DTYPE, path=training_folder, store=True, random_player=False)\n",
    "        \n",
    "        # ───────── quick evaluation for best-model tracking ───────────\n",
    "        quick_eval_msg = f\"{datetime.now().strftime('%H:%M:%S')} | Episode: {episode} | Running quick evaluation...\"\n",
    "        logger.log(quick_eval_msg)\n",
    "        \n",
    "        with suppress_output():\n",
    "            eval_obs = env.reset() # List of obs\n",
    "        \n",
    "        # Return the tensors for analysis\n",
    "        analyze_model_tensors(activation_logger, episode, eval_obs, model, split_dims=obs_states.get_dims(return_dict=False))\n",
    "\n",
    "        # Evaluate every EVALUTATION EVERY periods to improve the speed\n",
    "        if episode > 0 and episode % EVALUATION_EVERY == 0:\n",
    "                        \n",
    "            eval_episode_rewards = [0.0] * env.num_players\n",
    "            eval_dones = [False]\n",
    "            model.eval()\n",
    "        \n",
    "            while not all(eval_dones):\n",
    "                eval_action_list = epsilon_greedy(env, model, eval_obs, 0, env_actions, device, dtype=DTYPE)\n",
    "                eval_next_obs_list, eval_reward_list, eval_dones, _ = env.step(eval_action_list)\n",
    "                for i in range(env.num_players):\n",
    "                    eval_episode_rewards[i] += eval_reward_list[i]\n",
    "                eval_obs = eval_next_obs_list\n",
    "            \n",
    "            mean_eval_reward = np.mean(eval_episode_rewards)\n",
    "        \n",
    "        if episode > 0 and episode % CHECKPOINT_EVERY == 0:\n",
    "            if mean_eval_reward > best_reward:\n",
    "                best_reward = mean_eval_reward\n",
    "                model_name = f\"best_model_{datetime.now().strftime('%Y%m%d_%H%M%S')}_best{best_reward:.0f}_max{np.max(eval_episode_rewards):.0f}.pt\"\n",
    "                model.save_model(path=training_folder, filename=model_name)\n",
    "                best_model = deepcopy(model)\n",
    "            else:\n",
    "                model_name = f\"backup_model_{datetime.now().strftime('%Y%m%d_%H%M%S')}_mean{best_reward:.0f}_max{np.max(eval_episode_rewards):.0f}.pt\"\n",
    "                model.save_model(path=os.path.join(training_folder, 'backups'), filename=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OpBx9O9yzP66"
   },
   "source": [
    "## Dump to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YNkgBbUSzP66"
   },
   "outputs": [],
   "source": [
    "import onnx\n",
    "import json\n",
    "\n",
    "def onnx_dump(env, model, config, filename: str):\n",
    "    # dummy state\n",
    "    with suppress_output():\n",
    "        init_state = env.reset()[0].unsqueeze(0)\n",
    "\n",
    "    # Export to ONNX\n",
    "    torch.onnx.export(\n",
    "        model.cpu(),\n",
    "        args=init_state,\n",
    "        f=filename,\n",
    "        export_params=True,\n",
    "        opset_version=11,\n",
    "        do_constant_folding=True,\n",
    "        input_names=[\"input\"],\n",
    "        output_names=[\"output\"],\n",
    "        dynamic_axes={\"input\": {0: \"batch_size\"}, \"output\": {0: \"batch_size\"}},\n",
    "    )\n",
    "    onnx_model = onnx.load(filename)\n",
    "\n",
    "    meta = onnx_model.metadata_props.add()\n",
    "    meta.key = \"config\"\n",
    "    meta.value = json.dumps(config)\n",
    "\n",
    "    onnx.save(onnx_model, filename)\n",
    "\n",
    "    \n",
    "# ---------------------  SAVE / EXPORT ---------------------------------------\n",
    "final_model = best_model if best_model is not None else model  # choose best\n",
    "\n",
    "onnx_filename = os.path.join(training_folder, \"enhanced_doom_agent.onnx\")\n",
    "onnx_dump(env, final_model, PLAYER_CONFIG, onnx_filename)\n",
    "print(f\"Best network exported to {onnx_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4rZCsfQHFMu-"
   },
   "source": [
    "### Evaluation and Visualization\n",
    "\n",
    "In this final section, you can evaluate your trained agent, inspect its performance visually, and analyze reward components over time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ARC2K2k686nu"
   },
   "outputs": [],
   "source": [
    "plot_training_metrics(get_avg_reward(reward_history, episodes=0), q_loss_list, epsilon_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replay_episode(env, final_model.cpu(), device, DTYPE, store=True, random_player=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "jku_wad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
