{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hZLFJuX3_vID"
   },
   "source": [
    "# Deep Reinforcement Learning — Doom Agent (SS2025)\n",
    "\n",
    "Welcome to the last assignment for the **Deep Reinforcement Learning** course (SS2025). In this notebook, you\"ll implement and train a reinforcement learning agent to play **Doom**.\n",
    "\n",
    "You will:\n",
    "- Set up a custom VizDoom environment with shaped rewards\n",
    "- Train an agent using an approach of your choice\n",
    "- Track reward components across episodes\n",
    "- Evaluate the best model\n",
    "- Visualize performance with replays and GIFs\n",
    "- Export the trained agent to ONNX to submit to the evaluation server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mi5FBK3oi1lI",
    "outputId": "6a38df99-16cd-4a65-8bb0-ead70efcbd71"
   },
   "outputs": [],
   "source": [
    "# Clone repo\n",
    "#!git clone https://$token@github.com/gerkone/jku.wad.git\n",
    "#%cd jku.wad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oYTcQU_TiP06",
    "outputId": "9ca3b3f4-26b2-4c80-8aa2-9201e1841cdc"
   },
   "outputs": [],
   "source": [
    "# Install the dependencies\n",
    "#!pip install torch numpy matplotlib vizdoom portpicker gym onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import get_ipython\n",
    "get_ipython().run_line_magic('load_ext', 'autoreload')\n",
    "get_ipython().run_line_magic('autoreload', '2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "8VR5QUw8ieGh"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from collections import deque\n",
    "from copy import deepcopy\n",
    "import random\n",
    "from torch import nn\n",
    "from doom_arena import VizdoomMPEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents.utils import *\n",
    "from agents.visualizations import *\n",
    "from agents.helpers import *\n",
    "from agents.dqn import EfficientDQN, epsilon_greedy, soft_update_target_network, LargeDQN, SmallDQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using folder: runs/Testing\n"
     ]
    }
   ],
   "source": [
    "# Create folder for training\n",
    "training_id = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "#training_folder = os.path.join(\"runs\", training_id)\n",
    "training_folder = os.path.join(\"runs\", \"Testing\")\n",
    "\n",
    "os.makedirs(training_folder, exist_ok=True)\n",
    "print(\"Using folder:\", training_folder)\n",
    "\n",
    "# Define loggers\n",
    "logger = Logger(training_folder, also_print=True)\n",
    "activation_logger = ActivationLogger(training_folder, filename=\"activations.txt\", also_print=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pDz9CHjVzP61"
   },
   "source": [
    "## Environment configuration\n",
    "\n",
    "ViZDoom supports multiple visual buffers that can be used as input for training agents. Each buffer provides different information about the game environment, as seen from left to right:\n",
    "\n",
    "\n",
    "Screen\n",
    "- The default first-person RGB view seen by the agent.\n",
    "\n",
    "Labels\n",
    "- A semantic map where each pixel is tagged with an object ID (e.g., enemy, item, wall).\n",
    "\n",
    "Depth\n",
    "- A grayscale map showing the distance from the agent to surfaces in the scene.\n",
    "\n",
    "Automap\n",
    "- A top-down schematic view of the map, useful for global navigation tasks.\n",
    "\n",
    "![buffers gif](https://vizdoom.farama.org/_images/vizdoom-demo.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "nmNDlnmfzP62"
   },
   "outputs": [],
   "source": [
    "USE_GRAYSCALE = False  # ← flip to False for RGB\n",
    "\n",
    "PLAYER_CONFIG = {\n",
    "    # NOTE: \"algo_type\" defaults to POLICY in evaluation script!\n",
    "    \"algo_type\": \"QVALUE\",  # OPTIONAL, change to POLICY if using policy-based (eg PPO)\n",
    "    \"n_stack_frames\": 1, #4, # 1 # Temporal information\n",
    "    \"extra_state\": [\"labels\"], #[\"labels\"],#, \"automap\"], #[\"depth\", \"labels\", \"automap\"],\n",
    "    \"hud\": \"none\",\n",
    "    \"crosshair\": True,\n",
    "    \"screen_format\": 8 if USE_GRAYSCALE else 0,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "A3vdawvQzP62"
   },
   "outputs": [],
   "source": [
    "# TODO: environment training paramters\n",
    "N_STACK_FRAMES = 1 # 4 does not work yet!\n",
    "NUM_PLAYERS = 2 #3\n",
    "NUM_BOTS = 3 #1, #6\n",
    "EPISODE_TIMEOUT = 2000\n",
    "# TODO: model hyperparams\n",
    "GAMMA = 0.95\n",
    "EPISODES = 2 # 500\n",
    "BATCH_SIZE = 64\n",
    "REPLAY_BUFFER_SIZE = 20_000\n",
    "LEARNING_RATE = 5e-5\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_END = 0.1\n",
    "EPSILON_DECAY = 0.995 # 0.987\n",
    "N_EPOCHS =30\n",
    "TARGET_UPDATE_FREQ = 1000\n",
    "FRAME_SKIPPING = 0\n",
    "FRAME_SKIPPING_STOP = 100\n",
    "DEBUG = True\n",
    "PRINT_EVERY = 20\n",
    "VIDEO_DURING_TRAINING = False * DEBUG\n",
    "EVALUATION_EVERY = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OzTS4m2VzP63",
    "outputId": "369317da-3265-4f5c-b2f1-7990ca7d1f44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n",
      "Environment Seed: [1534 1670]\n",
      "Host 52542\n",
      "Player 52542\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)\n",
    "\n",
    "DTYPE = torch.float32\n",
    "\n",
    "obs_states = ExtraStates(PLAYER_CONFIG[\"extra_state\"] + [\"screen\"], num_frames = N_STACK_FRAMES)\n",
    "\n",
    "reward_fn = YourReward(num_players=NUM_PLAYERS)\n",
    "\n",
    "def build_env():\n",
    "    env = VizdoomMPEnv(\n",
    "        num_players=NUM_PLAYERS, # 4 is max\n",
    "        num_bots=NUM_BOTS,\n",
    "        bot_skill=1, # Increased difficulty\n",
    "        doom_map=\"TRNM\",  # ROOM = NOTE simple, small map; other options: TRNM, TRNMBIG\n",
    "        extra_state=PLAYER_CONFIG[\"extra_state\"],  # see info about states at the beginning of \"Environment configuration\" above\n",
    "        episode_timeout=EPISODE_TIMEOUT,\n",
    "        n_stack_frames=PLAYER_CONFIG[\"n_stack_frames\"],\n",
    "        crosshair=PLAYER_CONFIG[\"crosshair\"],\n",
    "        hud=PLAYER_CONFIG[\"hud\"],\n",
    "        screen_format=PLAYER_CONFIG[\"screen_format\"],\n",
    "        reward_fn=reward_fn,\n",
    "        seed = rng.integers(1, 2000, NUM_PLAYERS),\n",
    "        ticrate = 35*3,\n",
    "    )\n",
    "    return env\n",
    "\n",
    "env = build_env()\n",
    "with suppress_output(): # Reset to get player and game information\n",
    "    _ = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KkuIefaSzP64"
   },
   "source": [
    "## Agent\n",
    "\n",
    "Implement **your own agent** in the code cell that follows.\n",
    "\n",
    "* In `agents/dqn.py` and `agents/ppo.py` you’ll find very small **skeletons**—they compile but are meant only as reference or quick tests.  \n",
    "  Feel free to open them, borrow ideas, extend them, or ignore them entirely.\n",
    "* The notebook does **not** import those files automatically; whatever class you define in the next cell is the one that will be trained.\n",
    "* You may keep the DQN interface, switch to PPO, or try something else.\n",
    "* Tweak any hyper-parameters (`PLAYER_CONFIG`, ε-schedule, optimiser, etc.) and document what you tried.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "n7dWjwTdzP65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized model with 1222121 parameters!\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# Initialise your networks and training utilities\n",
    "# ================================================================\n",
    "env_actions = EnvActions(env, rng=rng)\n",
    "\n",
    "# main Q-network\n",
    "model = EfficientDQN(\n",
    "    input_dim = 0,\n",
    "    action_space=env_actions.action_space,\n",
    "    obs_state_infos=obs_states,\n",
    ").to(device, dtype=DTYPE)\n",
    "\n",
    "# Load a model from file\n",
    "#model = model.load_model(\"runs/20250621_182219/best_model_20250621_182229.pt\").to(device, dtype=DTYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_model  = deepcopy(model).to(device, dtype=DTYPE)\n",
    "optimizer  = torch.optim.AdamW(target_model.parameters(), lr=LEARNING_RATE, weight_decay=5e-4)\n",
    "scheduler  = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=20, eta_min=1e-8)\n",
    "loss_fn = nn.MSELoss()\n",
    "replay_buffer = deque(maxlen=REPLAY_BUFFER_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset environment\n",
    "with suppress_output():\n",
    "    obs_test: list = env.reset()\n",
    "    \n",
    "for player in range(len(obs_test)):\n",
    "    plot_images(obs_test[player], obs_states.get_dims())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate 60 steps\n",
    "STEPS = 30\n",
    "\n",
    "for i in range(STEPS):\n",
    "    actions = env_actions.get_random_action(NUM_PLAYERS)\n",
    "    #actions = env_actions.get_action_value(1, NUM_PLAYERS)\n",
    "    #actions = epsilon_greedy(env, model, obs_test, 0.5, env_actions, device, DTYPE, debug=True) # 50% random actions by model\n",
    "    #actions = epsilon_greedy(env, model, torch.rand_like(torch.stack(obs_test)), 0, env_actions, device, DTYPE, debug=False) # model fully random\n",
    "    obs_test, reward = env.step(actions)[0:2]\n",
    "\n",
    "for player in range(env.num_players):\n",
    "    action_print = [actions] if isinstance(actions, int) else actions\n",
    "    plot_images(obs_test[player], obs_states.get_dims(), **{\"title\": f\"Reward: {reward[player]:.1f} | Action: {env_actions.get_action_name(action_print[player])}\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of Reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_idx = rng.integers(env.num_players)\n",
    "\n",
    "player_game_vars = env.envs[player_idx].unwrapped._game_vars\n",
    "player_game_vars_pre = env.envs[player_idx].unwrapped._game_vars_pre\n",
    "reward = reward_fn(0, player_game_vars, player_game_vars_pre, 0)\n",
    "print(f\"Reward: {np.sum(reward)} | {reward}\")\n",
    "for key, value_now, value_old in zip(player_game_vars, player_game_vars.values(), player_game_vars_pre.values()):   \n",
    "    print(f\"{key+':':<25}{value_now:.0f} | {value_old:.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mihj8Q5xzP65"
   },
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_model_tensors(episode: int, obs_clean: tuple[torch.Tensor], model: nn.Module, model_sequence: list = [None, 0, 1, 1], print_once: bool = False):\n",
    "    rand_tensor = torch.rand(size=torch.stack(obs_clean).shape).to(\"cpu\").split(obs_states.get_dims(return_dict=False), dim=1)\n",
    "    \n",
    "    value, advantage = activation_logger.log_model_activations(rand_tensor, model, model_sequence, episode=episode, return_activations_from_idx=-2, print_once=print_once)\n",
    "    q_values = (value + advantage - advantage.mean(dim=1, keepdim=True))\n",
    "    actions = q_values.argmax(dim=1)\n",
    "    \n",
    "    activation_logger.analyze_activations(q_values, episode, title=\"Qvalues\", print_once=print_once)\n",
    "    activation_logger.analyze_activations(actions, episode, title=\"Actions\", print_once=print_once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(obs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_model_tensors(-1, obs_test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------  TRAINING LOOP  ----------------------\n",
    "\n",
    "# Training settings\n",
    "q_loss_list, epsilon_history = [], []\n",
    "reward_history = {player: [] for player in range(env.num_players)}\n",
    "best_reward = float(\"-inf\")\n",
    "best_model = None\n",
    "\n",
    "epsilon = EPSILON_START \n",
    "steps_done = 0\n",
    "\n",
    "\n",
    "with TqdmProgress(total=EPISODES) as progress_bar:\n",
    "    for episode in progress_bar:\n",
    "        \n",
    "        episode_action_counter = ActionCounter()\n",
    "        progress_bar.set_description(episode)\n",
    "        \n",
    "        with suppress_output():\n",
    "            #obs = torch.stack([env.reset() for env in environments.values()])\n",
    "            obs: torch.Tensor = env.reset()\n",
    "        \n",
    "        # Episode variables for each player\n",
    "        episode_metrics = {player: {\"frags\": 0, \"hits\": 0, \"damage_taken\": 0, \"movement\": 0, \"ammo_efficiency\": 0, \"survival\": 0, \"health_pickup\": 0} for player in range(env.num_players)}\n",
    "        episode_reward = {player: 0.0 for player in range(env.num_players)}\n",
    "\n",
    "        done = False\n",
    "        model.eval()\n",
    "        \n",
    "        episode_msg = f\"{datetime.now().strftime(\"%H:%M:%S\")} | Episode: {episode} | Steps done: {steps_done} | Gathering rollout (currently {len(replay_buffer)})\"\n",
    "        logger.log(episode_msg)\n",
    "        \n",
    "        # ───────── rollout ─────────────────────────────────────────────\n",
    "        while not done:\n",
    "            \n",
    "            # Skip Frames (Testing - I am aware that in the original paper they stacked the images. I want to get many more steps at the beginning)\n",
    "            act = epsilon_greedy(env, model, obs, epsilon, env_actions, device, DTYPE, episode_action_counter)\n",
    "            \n",
    "            max_skip_steps = max(FRAME_SKIPPING * (episode < FRAME_SKIPPING_STOP), 1)\n",
    "            for skip_steps in range(max_skip_steps):\n",
    "                next_obs, reward, done, _ = env.step(act)            \n",
    "                steps_done += 1\n",
    "\n",
    "            # ----- reward definition ----------------\n",
    "            for player_idx in range(env.num_players):\n",
    "                episode_reward[player_idx] += np.sum(reward[player_idx]) # Should be a list with one value\n",
    "                reward_components = reward_fn(None, env.envs[player_idx].unwrapped._game_vars, env.envs[player_idx].unwrapped._game_vars_pre, None)\n",
    "\n",
    "                if len(reward_components) >= 6:\n",
    "                    episode_metrics[player_idx][\"frags\"] += reward_components[0]\n",
    "                    episode_metrics[player_idx][\"hits\"] += reward_components[1]\n",
    "                    episode_metrics[player_idx][\"damage_taken\"] += reward_components[2]\n",
    "                    episode_metrics[player_idx][\"movement\"] += reward_components[3]\n",
    "                    episode_metrics[player_idx][\"ammo_efficiency\"] += reward_components[4]\n",
    "                    episode_metrics[player_idx][\"survival\"] += reward_components[5]\n",
    "                    episode_metrics[player_idx][\"health_pickup\"] += reward_components[6]\n",
    "            \n",
    "                # ----- buffer and environment handling ----------------\n",
    "                if env.num_players == 1:\n",
    "                    act = [act]\n",
    "                    \n",
    "                replay_buffer.append((obs[player_idx], act[player_idx], reward[player_idx], next_obs[player_idx], done))\n",
    "\n",
    "            obs = next_obs\n",
    "            \n",
    "        [reward_history[player_idx].append(reward) for player_idx, reward in episode_reward.items()] # Append rewards for every player\n",
    "        epsilon_history.append(epsilon)\n",
    "\n",
    "        # ───────── learning step (experience replay) ──────────────────\n",
    "        if len(replay_buffer) >= BATCH_SIZE:\n",
    "\n",
    "            model.train()\n",
    "            train_msg = f\"{datetime.now().strftime(\"%H:%M:%S\")} | Episode: {episode} | Training for {N_EPOCHS} epochs\"\n",
    "            logger.log(train_msg)\n",
    "            \n",
    "            for epoch in range(N_EPOCHS):\n",
    "\n",
    "                batch = random.sample(replay_buffer, BATCH_SIZE)\n",
    "                states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "                # Convert to tensors for training\n",
    "                states = torch.stack(states).to(device, DTYPE)\n",
    "                next_states = torch.stack(next_states).to(device, DTYPE)\n",
    "                actions = torch.tensor(actions, device=device)\n",
    "                rewards = torch.tensor(rewards, device=device, dtype=torch.float32) #.squeeze(1) # Added squeeze here\n",
    "                dones = torch.tensor(dones, device=device, dtype=torch.float32)\n",
    "\n",
    "                current_q = model(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "                \n",
    "                # Target q values\n",
    "                with torch.no_grad():\n",
    "                    next_q = target_model(next_states).max(1)[0] #.values\n",
    "                    target_q = rewards + GAMMA * next_q * (1 - dones)\n",
    "                \n",
    "                loss = loss_fn(current_q, target_q) # TODO: Check whether correct, did that in Deep Q Assignment\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                \n",
    "                q_loss_list.append(loss.item())\n",
    "            \n",
    "            # Update scheduler and epsilon\n",
    "            scheduler.step()\n",
    "            epsilon = max(EPSILON_END, epsilon * EPSILON_DECAY)\n",
    "            \n",
    "            progress_bar.update_step_count()\n",
    "\n",
    "            # Update target network        \n",
    "            target_update_msg = f\"{datetime.now().strftime(\"%H:%M:%S\")} | Episode: {episode} | Updating target network...\"\n",
    "            logger.log(target_update_msg)\n",
    "            \n",
    "            soft_update_target_network(model, target_model, tau=1e-3)\n",
    "            #hard_update_target_network(target_model, model)\n",
    "            \n",
    "        else:\n",
    "            train_fail_msg = f\"{datetime.now().strftime(\"%H:%M:%S\")} | Episode: {episode} | Replay buffer smaller than batchsize {len(replay_buffer)} {BATCH_SIZE}\"\n",
    "            logger.log(train_fail_msg, print_once=True)\n",
    "\n",
    "        # -------- logging ----------\n",
    "        avg_reward = get_avg_reward(reward_history, episodes=PRINT_EVERY, round=0)\n",
    "        avg_loss = np.mean(q_loss_list[-10:]) if q_loss_list else 0\n",
    "        stacked_episode_reward = np.array([np.round(reward, 0) for reward in episode_reward.values()])\n",
    "        \n",
    "        action_counts = episode_action_counter.get_counts()\n",
    "\n",
    "        # Rewards and losses\n",
    "        reward_msg = f\"{datetime.now().strftime(\"%H:%M:%S\")} | Episode: {episode} | Rewards:\\n\"\n",
    "        reward_msg += f\"\\tReward: {stacked_episode_reward} | Avg Reward: {avg_reward} | Loss: {avg_loss:.4f} | ε: {epsilon:.3f} | LR: {scheduler.get_last_lr()[0]:.2e}\"\n",
    "        reward_msg += f\"\\n\\tMetrics - {[f'{metric}: {value}' for metric, value in get_average_result(episode_metrics).items()]}\"\n",
    "        reward_msg += f\"\\n\\tActions - {', '.join([f\"{k}: {v}\" for k, v in sorted(action_counts.items())])}\"\n",
    "        \n",
    "        logger.log(reward_msg, print_once = episode % PRINT_EVERY == 0)\n",
    "        \n",
    "        progress_bar.set_postfix({\n",
    "            \"Avg Reward\": f\"{avg_reward}\",\n",
    "            \"Loss\": f\"{avg_loss:.4f}\",\n",
    "            \"Epsilon\": f\"{epsilon:.2f}\"\n",
    "        })\n",
    "            \n",
    "        # Show the video \n",
    "        if VIDEO_DURING_TRAINING and episode % PRINT_EVERY == 0:\n",
    "            replay_msg = f\"{datetime.now().strftime(\"%H:%M:%S\")} | Episode: {episode} | Replaying animation...\"\n",
    "            logger.log(replay_msg)\n",
    "            with suppress_output():\n",
    "                replay_episode(env, target_model, device, DTYPE, path=training_folder, store=True, random_player=False)\n",
    "        \n",
    "        # ───────── quick evaluation for best-model tracking ───────────\n",
    "        quick_eval_msg = f\"{datetime.now().strftime(\"%H:%M:%S\")} | Episode: {episode} | Running quick evaluation...\"\n",
    "        logger.log(quick_eval_msg)\n",
    "        \n",
    "        with suppress_output():\n",
    "            eval_obs = env.reset() # List of obs\n",
    "        \n",
    "        # Return the tensors for analysis\n",
    "        analyze_model_tensors(episode, eval_obs, target_model)\n",
    "\n",
    "        # Evaluate every EVALUTATION EVERY periods to improve the speed\n",
    "        if episode % EVALUATION_EVERY == 0:\n",
    "                        \n",
    "            eval_episode_rewards = [0.0] * env.num_players\n",
    "            eval_done = False\n",
    "            model.eval()\n",
    "        \n",
    "            while not eval_done:\n",
    "                eval_action_list = epsilon_greedy(env, model, eval_obs, 0, env_actions, device, dtype=DTYPE)\n",
    "                eval_next_obs_list, eval_reward_list, eval_done, _ = env.step(eval_action_list)\n",
    "                for i in range(env.num_players):\n",
    "                    eval_episode_rewards[i] += eval_reward_list[i]\n",
    "                eval_obs = eval_next_obs_list\n",
    "            \n",
    "            mean_eval_reward = np.mean(eval_episode_rewards)\n",
    "            \n",
    "            if mean_eval_reward > best_reward:\n",
    "                best_reward = mean_eval_reward\n",
    "                model_name = \"best_model_\" + datetime.now().strftime(\"%Y%m%d_%H%M%S\") + \".pt\"\n",
    "                model.save_model(path=training_folder, filename=model_name)\n",
    "                best_model = deepcopy(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OpBx9O9yzP66"
   },
   "source": [
    "## Dump to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YNkgBbUSzP66"
   },
   "outputs": [],
   "source": [
    "import onnx\n",
    "import json\n",
    "\n",
    "def onnx_dump(env, model, config, filename: str):\n",
    "    # dummy state\n",
    "    init_state = env.reset()[0].unsqueeze(0)\n",
    "\n",
    "    # Export to ONNX\n",
    "    torch.onnx.export(\n",
    "        model.cpu(),\n",
    "        args=init_state,\n",
    "        f=filename,\n",
    "        export_params=True,\n",
    "        opset_version=11,\n",
    "        do_constant_folding=True,\n",
    "        input_names=[\"input\"],\n",
    "        output_names=[\"output\"],\n",
    "        dynamic_axes={\"input\": {0: \"batch_size\"}, \"output\": {0: \"batch_size\"}},\n",
    "    )\n",
    "    onnx_model = onnx.load(filename)\n",
    "\n",
    "    meta = onnx_model.metadata_props.add()\n",
    "    meta.key = \"config\"\n",
    "    meta.value = json.dumps(config)\n",
    "\n",
    "    onnx.save(onnx_model, filename)\n",
    "\n",
    "    \n",
    "# ---------------------  SAVE / EXPORT ---------------------------------------\n",
    "final_model = best_model if best_model is not None else model  # choose best\n",
    "\n",
    "onnx_filename = os.path.join(training_folder, \"enhanced_doom_agent_onnx\")\n",
    "onnx_dump(env, final_model, PLAYER_CONFIG, onnx_filename)\n",
    "print(f\"Best network exported to {onnx_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4rZCsfQHFMu-"
   },
   "source": [
    "### Evaluation and Visualization\n",
    "\n",
    "In this final section, you can evaluate your trained agent, inspect its performance visually, and analyze reward components over time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ARC2K2k686nu"
   },
   "outputs": [],
   "source": [
    "plot_training_metrics(get_avg_reward(reward_history, episodes=0), q_loss_list, epsilon_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_episode(env, final_model.cpu(), device, DTYPE, store=True, random_player=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "jku_wad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
